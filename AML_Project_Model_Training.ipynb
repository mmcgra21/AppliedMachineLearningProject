{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zBsKTNCAisGARZyhQ5c2lKYg8wApqJ6O","timestamp":1733250856250}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"UlyvKtBQD2An"}},{"cell_type":"markdown","source":["## Import required packages"],"metadata":{"id":"EhLjbtGMsBOg"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"E_rG2ZyPqh9T","executionInfo":{"status":"ok","timestamp":1733595844111,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"outputs":[],"source":["# import xgboost\n","import xgboost as xgb\n","# import lightgbm\n","import lightgbm as lgb\n","# import packages for hyperparameters tuning\n","from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","# sklearn packages\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import roc_auc_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.ensemble import RandomForestClassifier\n","# from feature_engine.selection import DropCorrelatedFeatures\n","from sklearn.svm import LinearSVC\n","# miscellaneous\n","import os.path\n","from pprint import pprint\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["## Switch to project directory"],"metadata":{"id":"PMIf05Gtxrop"}},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd drive/MyDrive/COMSW4995_32 AML/AML Final Project/home-credit-default-risk\n","%cd home-credit-default-risk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtiBlNiTqtdy","executionInfo":{"status":"ok","timestamp":1733595844111,"user_tz":300,"elapsed":21292,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}},"outputId":"84358cbb-1686-4d7d-9089-e080d803465b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Aq4j1LDPosstk7lY4v_Ck2m1_6P4wfJb/AML Final Project/home-credit-default-risk\n"]}]},{"cell_type":"markdown","source":["## Create directories if necessary"],"metadata":{"id":"Y_t_KLZOvUOY"}},{"cell_type":"code","source":["# Check if correlation_matrices directory exists, if not create directory\n","if not os.path.exists('correlation_matrices'):\n","    os.makedirs('correlation_matrices')\n","\n","# Check if models directory exists, if not create directory\n","if not os.path.exists('models'):\n","    os.makedirs('models')"],"metadata":{"id":"cjJ6A4AivXMK","executionInfo":{"status":"ok","timestamp":1733595851915,"user_tz":300,"elapsed":144,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Define model and search space"],"metadata":{"id":"28Hg0CaNr_wY"}},{"cell_type":"markdown","source":["\n","When attempting a new model, the below code is the only thing you need to change.\n","\n","- Model name (what should the model be saved as?)\n","- Correlation threshold (correlation threshold used to filter features)\n","- Number of search trials (for hyperparameter tuning)\n","- Define the search space (for hyperparameter tuning):\n","    - Hyperparameters to search over\n","    - Range of values to search over for each hyperparameter\n","- Define the model pipeline (what machine learning method(s) to use (e.g. XGBoost, Logistic Regression, etc.))\n","- For this problem set, we trained Logistic Regression, XGBoost, Random Forest, and LightGBM as shown below."],"metadata":{"id":"hNrGapW3sOrF"}},{"cell_type":"code","source":["# # XGBoost\n","# model_name = 'xgboost_wo_pca'\n","# num_search_trials = 50\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {\n","#         # 'n_components': hp.quniform('n_components', 5, X_train.shape[1], 1),\n","#         'max_depth': hp.quniform('max_depth', 3, 30, 1),\n","#         'gamma': hp.uniform ('gamma', 1, 9),\n","#         'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n","#         'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n","#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n","#         'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n","#         'seed': 0\n","#     }\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#             # ('pca', PCA(n_components=int(inp['n_components']), random_state=42)),\n","#             ('xgb', xgb.XGBClassifier(n_estimators=180, max_depth = int(inp['max_depth']), gamma = inp['gamma'],\n","#                                     reg_alpha = int(inp['reg_alpha']),min_child_weight=int(inp['min_child_weight']),\n","#                                     colsample_bytree=int(inp['colsample_bytree']), random_state=42))#, eval_metric=roc_auc_score, early_stopping_rounds=10)  )\n","#     ])\n","\n","# XGBoost\n","model_name = 'xgboost_plain'\n","num_search_trials = 1\n","correlation_threshold = 0\n","\n","def define_search_space():\n","    return {'seed': 0}\n","\n","def define_model(inp):\n","    return Pipeline([\n","            ('xgb', xgb.XGBClassifier(random_state=42))#, eval_metric=roc_auc_score, early_stopping_rounds=10)  )\n","    ])\n","\n","\n","\n","# # Logistic Regression\n","# model_name = 'logistic_regression_wo_pca'\n","# num_search_trials = 10\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {\n","#         # 'n_components': hp.quniform('n_components', 5, X.shape[1], 1),\n","#         'C': hp.uniform('C', 0.001, 100)\n","#     }\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#             # ('pca', PCA(n_components=int(inp['n_components']), random_state=42)),\n","#             ('logistic', LogisticRegression(penalty='l1', solver='saga', C=inp['C'], random_state=42))\n","#     ])\n","\n","# # Logistic Regression\n","# model_name = 'logistic_regression_plain'\n","# num_search_trials = 1\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {'seed': 0}\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#             ('logistic', LogisticRegression(penalty='l1', solver='saga', random_state=42))\n","#     ])\n","\n","# # Random Forest\n","# model_name = 'random_forest_plain'\n","# num_search_trials = 1\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {'seed': 0}\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#         ('random_forest', RandomForestClassifier(\n","#             n_estimators=100,  # Number of trees\n","#             max_depth=10,\n","#             random_state=42,\n","#             class_weight='balanced'  # Handle class imbalance\n","#         ))\n","#     ])\n","\n","# # Random Forest with hyperparameter tuning\n","# model_name = 'random_forest_tuned'\n","# num_search_trials = 5\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {\n","#         'n_estimators': hp.quniform('n_estimators', 50, 500, 10),\n","#         'max_depth': hp.quniform('max_depth', 3, 30, 1),\n","#         'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),\n","#         'seed': 0\n","#     }\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#         ('random_forest', RandomForestClassifier(\n","#             n_estimators=int(inp['n_estimators']),  # Number of trees\n","#             max_depth=int(inp['max_depth']),  # Maximum tree depth\n","#             min_samples_split=int(inp['min_samples_split']),  # Minimum samples to split a node\n","#             random_state=42,\n","#             class_weight='balanced'  # Handle class imbalance\n","#         ))\n","#     ])\n","\n","# # LightGBM\n","# model_name = 'lightgbm_tuned'\n","# num_search_trials = 10\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {\n","#         'max_depth': hp.quniform('max_depth', 3, 30, 1),\n","#         'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","#         'num_leaves': hp.quniform('num_leaves', 20, 200, 1),\n","#         'min_child_samples': hp.quniform('min_child_samples', 1, 50, 1),\n","#         'subsample': hp.uniform('subsample', 0.5, 1),\n","#         'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n","#         'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n","#         'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n","#         'seed': 0\n","#     }\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#         ('lgb', lgb.LGBMClassifier(\n","#             n_estimators=180,\n","#             max_depth=int(inp['max_depth']),\n","#             learning_rate=inp['learning_rate'],\n","#             num_leaves=int(inp['num_leaves']),\n","#             min_child_samples=int(inp['min_child_samples']),\n","#             subsample=inp['subsample'],\n","#             colsample_bytree=inp['colsample_bytree'],\n","#             reg_alpha=inp['reg_alpha'],\n","#             reg_lambda=inp['reg_lambda'],\n","#             random_state=42\n","#         ))\n","#     ])\n"],"metadata":{"id":"S059SMl-r02v","executionInfo":{"status":"ok","timestamp":1733595853021,"user_tz":300,"elapsed":145,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# # lightGBM model\n","# model_name = 'lightgbm_tuned'\n","# num_search_trials = 10\n","# correlation_threshold = 0\n","\n","# def define_search_space():\n","#     return {\n","#         'max_depth': hp.quniform('max_depth', 3, 30, 1),\n","#         'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","#         'num_leaves': hp.quniform('num_leaves', 20, 200, 1),\n","#         'min_child_samples': hp.quniform('min_child_samples', 1, 50, 1),\n","#         'subsample': hp.uniform('subsample', 0.5, 1),\n","#         'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n","#         'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n","#         'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n","#         'seed': 0\n","#     }\n","\n","# def define_model(inp):\n","#     return Pipeline([\n","#         ('lgb', lgb.LGBMClassifier(\n","#             n_estimators=180,\n","#             max_depth=int(inp['max_depth']),\n","#             learning_rate=inp['learning_rate'],\n","#             num_leaves=int(inp['num_leaves']),\n","#             min_child_samples=int(inp['min_child_samples']),\n","#             subsample=inp['subsample'],\n","#             colsample_bytree=inp['colsample_bytree'],\n","#             reg_alpha=inp['reg_alpha'],\n","#             reg_lambda=inp['reg_lambda'],\n","#             random_state=42\n","#         ))\n","#     ])\n"],"metadata":{"id":"Cc3igLqwUBGb","executionInfo":{"status":"ok","timestamp":1733595853863,"user_tz":300,"elapsed":139,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Correlation thresholding helper function"],"metadata":{"id":"fxxR5U6lwzit"}},{"cell_type":"markdown","source":["If `correlation_threshold` > 0, then for each pair of highly correlated features (if the absolute value of their correlation is greater than `correlation_threshold`), remove one of those features."],"metadata":{"id":"J9teJTvvCW7x"}},{"cell_type":"code","source":["def remove_correlated_features(X, thresh, corr_matrix):\n","    df = pd.DataFrame(X)\n","    if corr_matrix is None:\n","        corr_matrix = df.corr()\n","    cols = df.columns\n","    for i in range(len(corr_matrix.columns)):\n","        for j in range(i+1, len(corr_matrix.columns)):\n","            col1 = corr_matrix.columns[i]\n","            col2 = corr_matrix.columns[j]\n","            correlation = corr_matrix.iloc[i, j]\n","            if abs(correlation) > thresh:\n","                if col2 in cols:\n","                    cols = cols.drop(col2)\n","    return cols, corr_matrix"],"metadata":{"id":"6jBx3aDd46yJ","executionInfo":{"status":"ok","timestamp":1733595856481,"user_tz":300,"elapsed":129,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Train models on each dataset"],"metadata":{"id":"3EJh_gfWtKQs"}},{"cell_type":"code","source":["models = dict()\n","models['hyperparameters'] = dict()\n","models['model'] = dict()\n","models['auroc_scores'] = dict()\n","models['IDs'] = dict()\n","models['preprocessor'] = dict()\n","models['columns'] = dict()\n","\n","application_train = pd.read_csv('train_val_data/train.csv')\n","id_and_target = application_train[['SK_ID_CURR', 'TARGET']]\n","\n","model_file = 'models/'+model_name+'_models.pickle'\n","if os.path.exists(model_file):\n","    with open(model_file, 'rb') as handle:\n","        models = pickle.load(handle)\n","\n","files = ['application_train.csv','bureau.csv','previous_application.csv','POS_CASH_balance.csv','installments_payments.csv','credit_card_balance.csv']\n","for file in files:\n","    name = file.split('.')[0]\n","    print(name)\n","    if name in models['auroc_scores']:\n","        print('    Already trained')\n","        continue\n","    if name == 'application_train':\n","        models['IDs'][name] = list(application_train['SK_ID_CURR'].unique())\n","\n","        X = application_train.drop(columns=['SK_ID_CURR','TARGET'])\n","        y = application_train['TARGET']\n","\n","        # Identify numeric and categorical columns\n","        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n","        categorical_features = X.select_dtypes(include=['object']).columns\n","\n","        # Create transformers for numeric and categorical features\n","        numeric_transformer = Pipeline([\n","            # ('imputer', KNNImputer()),\n","            ('imputer', SimpleImputer(strategy='median')),\n","            ('scaler', StandardScaler())\n","        ])\n","\n","        categorical_transformer = Pipeline([\n","            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","        ])\n","\n","        # Use ColumnTransformer to apply transformations to the correct columns\n","        preprocessor = ColumnTransformer(\n","            transformers=[\n","                ('num', numeric_transformer, numeric_features),\n","                ('cat', categorical_transformer, categorical_features)\n","            ]\n","        )\n","\n","        # Split the data into training and validation sets\n","        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","        X_train = preprocessor.fit_transform(X_train)\n","        X_val = preprocessor.transform(X_val)\n","    else:\n","        df = pd.read_csv('transformed/'+file.split('.')[0]+'_transformed.csv')\n","        models['IDs'][name] = list(df['SK_ID_CURR'].unique())\n","        df = df.merge(id_and_target, on='SK_ID_CURR', how='inner')\n","\n","        X = df.drop(columns=['SK_ID_CURR','TARGET'])\n","        y = df['TARGET']\n","\n","        transformer = Pipeline([\n","            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","            ('scaler', StandardScaler())\n","        ])\n","\n","        # Use ColumnTransformer to apply transformations to the correct columns\n","        preprocessor = ColumnTransformer(\n","            transformers=[\n","                ('transformer', transformer, X.columns)\n","            ]\n","        )\n","\n","        # Split the data into training and validation sets\n","        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","        X_train = preprocessor.fit_transform(X_train)\n","        X_val = preprocessor.transform(X_val)\n","\n","    if correlation_threshold > 0:\n","        if os.path.exists('correlation_matrices/'+name+'_correlation_matrix.csv'):\n","            corr_matrix = pd.read_csv('correlation_matrices/'+name+'_correlation_matrix.csv')\n","            models['columns'][name], _ = remove_correlated_features(X_train, correlation_threshold, corr_matrix)\n","        else:\n","            models['columns'][name], corr_matrix = remove_correlated_features(X_train, correlation_threshold, None)\n","            corr_matrix.to_csv('correlation_matrices/'+name+'_correlation_matrix.csv', index=False)\n","    else:\n","        models['columns'][name] = pd.DataFrame(X_train).columns\n","    X_train = X_train[:, models['columns'][name]]\n","    X_val = X_val[:, models['columns'][name]]\n","\n","    models['preprocessor'][name] = preprocessor\n","\n","    def objective(space):\n","        pipe = define_model(space)\n","        pipe.fit(X_train, y_train)\n","        pred = pipe.predict_proba(X_val)[:, 1]\n","        auroc = roc_auc_score(y_val, pred)\n","        return {'loss': -auroc, 'status': STATUS_OK }\n","\n","    space = define_search_space()\n","\n","    trials = Trials()\n","\n","    models['hyperparameters'][name] = fmin(fn = objective,\n","                                             space = space,\n","                                             algo = tpe.suggest,\n","                                             max_evals = num_search_trials,\n","                                             trials = trials)\n","    # pprint(models['hyperparameters'][name])\n","\n","    models['preprocessor'][name] = preprocessor\n","    models['model'][name] = define_model(models['hyperparameters'][name])\n","    models['model'][name].fit(np.concatenate((X_train, X_val), axis=0), np.concatenate((y_train, y_val), axis=0))\n","    models['auroc_scores'][name] = -trials.best_trial['result']['loss']\n","    with open(model_file, 'wb') as handle:\n","        pickle.dump(models, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# models['space'] = space\n","models['num_search_trials'] = num_search_trials\n","models['name'] = model_name\n","models['correlation_threshold'] = correlation_threshold\n","# pprint(models)\n","with open(model_file, 'wb') as handle:\n","    pickle.dump(models, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GKuJCagquEZ","executionInfo":{"status":"ok","timestamp":1733596054040,"user_tz":300,"elapsed":196206,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}},"outputId":"4f2326bb-736a-4443-9864-ac320eba9f91"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["application_train\n","100%|██████████| 1/1 [00:16<00:00, 16.07s/trial, best loss: -0.742553648811635]\n","bureau\n","100%|██████████| 1/1 [00:06<00:00,  6.16s/trial, best loss: -0.647920596705094]\n","previous_application\n","100%|██████████| 1/1 [00:20<00:00, 20.39s/trial, best loss: -0.6628085425174505]\n","POS_CASH_balance\n","100%|██████████| 1/1 [00:06<00:00,  6.09s/trial, best loss: -0.5820431775692597]\n","installments_payments\n","100%|██████████| 1/1 [00:04<00:00,  4.49s/trial, best loss: -0.617462813416681]\n","credit_card_balance\n","100%|██████████| 1/1 [00:03<00:00,  3.28s/trial, best loss: -0.6348568691029306]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"A1CF1ARSXYLB"},"execution_count":null,"outputs":[]}]}