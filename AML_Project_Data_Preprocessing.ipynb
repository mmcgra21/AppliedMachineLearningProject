{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhj/sRif3BFjBuWTaWrqzm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"uimcon6ow6mQ"}},{"cell_type":"markdown","source":["## Import required packages"],"metadata":{"id":"KByRFhqBw9X8"}},{"cell_type":"code","source":["# import xgboost\n","import xgboost as xgb\n","# import packages for hyperparameters tuning\n","from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","# sklearn packages\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import roc_auc_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.svm import LinearSVC\n","# miscellaneous\n","import os.path\n","from pprint import pprint\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"veljU0URw3Ab","executionInfo":{"status":"ok","timestamp":1733593284991,"user_tz":300,"elapsed":9009,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Switch to project directory"],"metadata":{"id":"r3v40sVSxizk"}},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd drive/MyDrive/COMSW4995_32 AML/AML Final Project/home-credit-default-risk\n","%cd home-credit-default-risk"],"metadata":{"id":"oDBi731KxXSp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733593306151,"user_tz":300,"elapsed":21163,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}},"outputId":"045bd87e-ad93-4eeb-f627-60ada521a9fd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Aq4j1LDPosstk7lY4v_Ck2m1_6P4wfJb/AML Final Project/home-credit-default-risk\n"]}]},{"cell_type":"markdown","source":["## Create directories if necessary"],"metadata":{"id":"KwNuu4YMvnmO"}},{"cell_type":"code","source":["# Check if train_val_data directory exists, if not create directory\n","if not os.path.exists('train_val_data'):\n","    os.makedirs('train_val_data')\n","\n","# Check if transformed directory exists, if not create directory\n","if not os.path.exists('transformed'):\n","    os.makedirs('transformed')"],"metadata":{"id":"wnnWL3FDvqLk","executionInfo":{"status":"ok","timestamp":1733593306694,"user_tz":300,"elapsed":546,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Create labeled test data to evaluate the model by splitting the training dataset."],"metadata":{"id":"dltpixyEw3zB"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"KA5lxqLQwh5R","executionInfo":{"status":"ok","timestamp":1733593344107,"user_tz":300,"elapsed":37414,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"outputs":[],"source":["application_train = pd.read_csv('application_train.csv')\n","X = application_train.drop(columns=['TARGET'])\n","y = application_train['TARGET']\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","application_train = X_train\n","application_train['TARGET'] = y_train\n","application_test = X_val\n","application_test['TARGET'] = y_val\n","application_train.to_csv('train_val_data/train.csv', index=False)\n","application_test.to_csv('train_val_data/test.csv', index=False)"]},{"cell_type":"markdown","source":["## Transform supplementary datasets"],"metadata":{"id":"pi6k4dfVxUpk"}},{"cell_type":"markdown","source":["Transform supplementary dataset by creating new features that aggreagate over `SK_ID_CURR` (the primary key of the main dataset (`application`))."],"metadata":{"id":"ctemZ78myF-B"}},{"cell_type":"markdown","source":["For each of the supplementary datasets in order to merge them with a label, we need to create features such that there is one row per `SK_ID_CURR` which is the primary key of the main dataset..\n","\n","After research and analyzing the columns and their meanings, I came up with the following three types of transforms:\n","\n","1. `ohe`: For categorical features, do a one hot encoding of each their unique values and count the number of credits per `SK_ID_CURR` for each unique value\n","2. `time`: For features that represent time in days, create a column for the first (min), last (max), and then number in past 6 month (>= -182), year (>= -365), and 5 years (>= -365*5)\n","3. `amount`: For features that represent an amount or a count, take the max, mean, min, and sum.\n","\n","In order to simplify the assignment of a feature to its type of transform we take advantage of the consistency in naming features and there datatypes:\n","\n","- We notice that all features that are categorical are of type `object`, while all others are numerical (`int64` or `float64`).\n","- We notice of the numerical types, the features that represent time have the substring `DAYS` in the name of the feature.\n","\n","Using this we create the following function to transform the supplementary datasets."],"metadata":{"id":"dEZ90pMT43qH"}},{"cell_type":"code","source":["# Define transformation functions\n","def transform_ohe(df, column):\n","    # One-hot encode and aggregate counts for each category\n","    tmp = pd.get_dummies(df[column]).groupby(df['SK_ID_CURR']).sum()\n","    tmp.columns = [column+'_'+col for col in tmp.columns]\n","    return tmp\n","\n","def transform_time(df, column):\n","    # Calculate time-based aggregates\n","    grouped = df.groupby('SK_ID_CURR')[column]\n","    last_applied = grouped.max()\n","    first_applied = grouped.min()\n","    num_last_6mo = grouped.apply(lambda x: (x >= -182).sum())\n","    num_last_year = grouped.apply(lambda x: (x >= -365).sum())\n","    num_last_5years = grouped.apply(lambda x: (x >= -365*5).sum())\n","    num_future = grouped.apply(lambda x: (x >= 0).sum())\n","    return pd.DataFrame({\n","        f'{column}_LAST': last_applied,\n","        f'{column}_FIRST': first_applied,\n","        f'{column}_COUNT_PAST_6MO': num_last_6mo,\n","        f'{column}_COUNT_PAST_YR': num_last_year,\n","        f'{column}_COUNT_PAST_5YRS': num_last_5years,\n","        f'{column}_COUNT_FUTURE': num_future\n","\n","    })\n","\n","def transform_amount(df, column):\n","    # Calculate amount-based aggregates\n","    grouped = df.groupby('SK_ID_CURR')[column]\n","    return pd.DataFrame({\n","        f'{column}_MAX': grouped.max(),\n","        f'{column}_MEAN': grouped.mean(),\n","        f'{column}_MIN': grouped.min(),\n","        f'{column}_SUM': grouped.sum()\n","    })\n","\n","def get_transform_dict(df):\n","    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_features = df.select_dtypes(include=['object']).columns\n","\n","    # remove any column names that contain '_ID_' in it\n","    numerical_features = [col for col in numeric_features if '_ID_' not in col]\n","\n","    transform_dict = dict()\n","    for col in numeric_features:\n","        if 'DAYS' in col:\n","            transform_dict[col] = 'time'\n","        else:\n","            transform_dict[col] = 'amount'\n","    for col in categorical_features:\n","        transform_dict[col] = 'ohe'\n","    return transform_dict\n","\n","# Apply transformations based on the dictionary\n","def transform_df(file, df):\n","    if df is None:\n","        df = pd.read_csv(file)\n","    transform_dict = get_transform_dict(df)\n","    result = []\n","    for column, transform_type in transform_dict.items():\n","        if transform_type == 'ohe':\n","            result.append(transform_ohe(df, column))\n","        elif transform_type == 'time':\n","            result.append(transform_time(df, column))\n","        elif transform_type == 'amount':\n","            result.append(transform_amount(df, column))\n","    # Concatenate all results on SK_ID_CURR to form the final dataframe\n","    df_transformed = pd.concat(result, axis=1).reset_index()\n","    name = file.split('.')[0]\n","    df_transformed.to_csv('transformed/'+name+'_transformed.csv', index=False)\n","    return df_transformed"],"metadata":{"id":"uTdreGrmxxRO","executionInfo":{"status":"ok","timestamp":1733593344108,"user_tz":300,"elapsed":5,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Loop over supplementary datasets and transform them, saving a new .csv in the *transformed* folder."],"metadata":{"id":"hPosFlw0y0PY"}},{"cell_type":"code","source":["files = ['previous_application.csv','POS_CASH_balance.csv','installments_payments.csv','credit_card_balance.csv']\n","for file in files:\n","    if not os.path.isfile('transformed/'+file.split('.')[0]+'_transformed.csv'):\n","        df = transform_df(file, None)"],"metadata":{"id":"CZY091TtysHi","executionInfo":{"status":"ok","timestamp":1733594939498,"user_tz":300,"elapsed":1595394,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Note that *bureau.csv* and *bureau_balance.csv* are not included above, that is because *bureau_balance.csv* does not have the `SK_ID_CURR`, and thus first we will have to create new features that aggreagate over `SK_ID_BUREAU` (the primary key of `bureau`), and then merge it into `bureau` before doing the final transformation of `bureau`."],"metadata":{"id":"JcMp38Aj2jR4"}},{"cell_type":"markdown","source":["In order to merge `bureau_balance` into `bureau`, while maintaining `SK_ID_BUREAU` a primary key in `bureau`, we have to make the `SK_ID_BUREAU` in `bureau_balance` unique without losing all the data stored in that table. Fortunately, there are only two other columns, which we can group by and compute new features that have `SK_ID_BUREAU` unique and which we will then merge into `bureau`.\n","\n","After analyzing the features in `bureau_balance`, I came up with 4 new features:\n","- MAX_NUM_MONTHS_PAST_DUE: Maximum number of months past due at any particular time for this past credit\n","- MOST_RECENT_STATUS: Most recent status of this past credit\n","- TOTAL_NUM_MONTHS_PAST_DUE: Total number of months past due throughout the duration of this credit\n","- AVG_MONTHS_PAST_DUE: Average number of months past due throughout the duration of this credit\n","\n","Note that these values are not exact since there are some monthly updates that have unknown statuses, which we ignore here. They are however good enough estimates."],"metadata":{"id":"jXG3C99z37OA"}},{"cell_type":"code","source":["# Load in datasets\n","bureau = pd.read_csv('bureau.csv')\n","bureau_balance = pd.read_csv('bureau_balance.csv')\n","# Remove all rows with STATUS == 'X' or 'C'\n","bureau_balance_new = bureau_balance.copy()\n","bureau_balance_new = bureau_balance_new[bureau_balance_new['STATUS'] != 'X']\n","bureau_balance_new = bureau_balance_new[bureau_balance_new['STATUS'] != 'C']\n","# Convert the STATUS column to int\n","bureau_balance_new['STATUS'] = bureau_balance_new['STATUS'].astype(int)\n","# Create new DataFrame bureau_balance_unique that has one column called\n","# 'SK_ID_BUREAU' which is the unique of bureau_balance_new['SK_ID_BUREAU']\n","# Create new column MAX_NUM_MONTHS_PAST_DUE which is the max STATUS in bureau_balance_new\n","bureau_balance_unique = bureau_balance_new.groupby('SK_ID_BUREAU').agg(\n","    MAX_NUM_MONTHS_PAST_DUE = ('STATUS', 'max')\n",")\n","# Create new column MOST_RECENT_STATUS which is the STATUS associated with the most recent column in bureau_balance_new (where MONTHS_BALANCE is max)\n","bureau_balance_unique['MOST_RECENT_STATUS'] = bureau_balance_new.sort_values('MONTHS_BALANCE', ascending=False).groupby('SK_ID_BUREAU').first()['STATUS']\n","# Edit bureau_balance_new STATUS column by converting any STATUS greater than 0 to 1\n","bureau_balance_new.loc[bureau_balance_new['STATUS'] > 0,'STATUS'] = 1\n","# Create new column TOTAL_NUM_MONTHS_PAST_DUE which is the sum of the STATUS in bureau_balance_new\n","# Create new column AVG_NUM_MONTHS_PAST_DUE which is the average of the STATUS in bureau_balance_new\n","bureau_balance_unique[['TOTAL_NUM_MONTHS_PAST_DUE','AVG_NUM_MONTHS_PAST_DUE']] = bureau_balance_new.groupby('SK_ID_BUREAU').agg(\n","    TOTAL_NUM_MONTHS_PAST_DUE = ('STATUS', 'sum'),\n","    AVG_NUM_MONTHS_PAST_DUE = ('STATUS', 'mean')\n",")\n","# Reset index\n","bureau_balance_unique = bureau_balance_unique.reset_index()\n","# Merge into bureau\n","bureau = bureau.merge(bureau_balance_unique, on='SK_ID_BUREAU', how='left')"],"metadata":{"id":"2llIdozo36vQ","executionInfo":{"status":"ok","timestamp":1733594968620,"user_tz":300,"elapsed":29126,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now we can transform our new `bureau` dataset as we did with the other supplementary datasets above."],"metadata":{"id":"FC-ylM654qHd"}},{"cell_type":"code","source":["df = transform_df('bureau.csv', bureau)"],"metadata":{"id":"wmBaD_er2i9W","executionInfo":{"status":"ok","timestamp":1733595641162,"user_tz":300,"elapsed":672546,"user":{"displayName":"Michael McGrath","userId":"13792584385567558674"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xEv9tF0HNgY3"},"execution_count":null,"outputs":[]}]}